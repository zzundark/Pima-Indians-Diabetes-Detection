{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import msvcrt\n",
    "import sys\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#데이터셋 경로 위치\n",
    "DATASET_PATH      = \"./diabetes.csv\"\n",
    "#텐서보드 저장경로\n",
    "LOGS_PATH\t= \"logs/\"\n",
    "#모델의 입력 차원=> 현재 input pipeline에서 자동 조정 \n",
    "NUM_DIM=8\n",
    "NUM_LABELS\t\t= 2\n",
    "#batch size 모두 0 => input pipeline에서 자동 조정 \n",
    "TRAIN_BATCH_SIZE\t= 0\n",
    "VALID_BATCH_SIZE\t= 0\n",
    "TEST_BATCH_SIZE\t\t= 0\n",
    "#epoch \n",
    "MAX_EPOCH=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(라벨,콘텐츠(etc 음성,텍스트)) 데이터 읽기\t=> trainset,validset, test으로 나뉘어짐 / ratio는 다 합쳐서 1이 되어야 됌 \n",
    "def read_label_file(file='./diabetes.csv',train_split_ratio=0.6,valid_split_ratio=0.2,test_split_ratio=0.2):\n",
    "\tdf = pd.read_csv(file, encoding='latin-1')\n",
    "\tlabel=df.Outcome.tolist()\n",
    "\tcontents=[]\n",
    "\tfor row in df.iterrows():\n",
    "\t\tindex, data = row\n",
    "\t\tcontents.append(data[0:8].tolist())\n",
    "\n",
    "\tsize=len(label)\n",
    "\n",
    "\tsum_ratio=train_split_ratio+valid_split_ratio+test_split_ratio\n",
    "\t\n",
    "\tassert  abs(1.0000 - sum_ratio) <= 0.0001, 'ratio의 합(%f)이 1이 아닙니다.'%(sum_ratio) \n",
    "    \n",
    "\ttrain_size=int(size*train_split_ratio)\n",
    "\ttrain_split_range=train_size\n",
    "\ttrain_contents=contents[0:train_split_range]\n",
    "\ttrain_label=label[0:train_split_range]\n",
    "\t\n",
    "\tvalid_size=int(size*valid_split_ratio)\n",
    "\tvalid_split_range=valid_size+train_split_range\n",
    "\tvalid_contents=contents[train_split_range:valid_split_range]\n",
    "\tvalid_label=label[train_split_range:valid_split_range]\n",
    "\t\n",
    "\ttest_size=int(size*test_split_ratio)\n",
    "\ttest_split_range=test_size+valid_split_range\n",
    "\ttest_contents=contents[valid_split_range:test_split_range]\n",
    "\ttest_label=label[valid_split_range:test_split_range]\n",
    "\t\n",
    "\t\n",
    "\treturn train_size,valid_size,test_size,train_contents,train_label,valid_contents,valid_label,test_contents,test_label\n",
    "\n",
    "def preprocess(data,label,num_class):\n",
    "\n",
    "\t#data를 sequences로 변환\n",
    "\tcontents=data\n",
    "\t\n",
    "\t# string을 tensor로 변환, type=float32\n",
    "\tlabel = tf.one_hot(label,depth=num_class,on_value=1,off_value=0,axis=-1)\n",
    "\treturn contents,label\n",
    "\t\t\n",
    "def create_queue(contents,labels):\n",
    "\n",
    "\t#입력 큐 생성, 데이터 섞기(shuffle)=true \n",
    "\tinput_queue = tf.train.slice_input_producer(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t[contents, labels],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tshuffle=True)\n",
    "\t#입력 큐 대입(데이터,라벨)\n",
    "\tcontent = input_queue[0]\n",
    "\tlabel = input_queue[1]\n",
    "\t\n",
    "\treturn content,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input pipeline ready\n"
     ]
    }
   ],
   "source": [
    "#read_label_file_split(dataset_path)\n",
    "total_train_size,total_valid_size,total_test_size,train_contents,train_labels,valid_contents,valid_labels,test_contents,test_labels = read_label_file(DATASET_PATH,0.6,0.2,0.2)\n",
    "#batch size  = 전체 데이터셋 수의 10%으로 설정 \n",
    "if(TRAIN_BATCH_SIZE==0):\n",
    "    TRAIN_BATCH_SIZE=(int)(total_train_size*0.1)\n",
    "if(VALID_BATCH_SIZE==0):\n",
    "    VALID_BATCH_SIZE=(int)(total_valid_size*0.1) \n",
    "if(TEST_BATCH_SIZE==0):\n",
    "    TEST_BATCH_SIZE=(int)(total_test_size*0.1) \n",
    "\n",
    "#train,valid,test set 각각 전처리\n",
    "train_contents,train_labels=preprocess(train_contents,train_labels,NUM_LABELS)\n",
    "valid_contents,valid_labels=preprocess(valid_contents,valid_labels,NUM_LABELS)\n",
    "test_contents,test_labels=preprocess(test_contents,test_labels,NUM_LABELS)\n",
    "\n",
    "\n",
    "\n",
    "#큐 생성  => train,valid,test  => train,valid,test set 각각 큐 생성 \n",
    "train_content, train_label=create_queue(train_contents,train_labels)\n",
    "valid_content, valid_label=create_queue(valid_contents,valid_labels)\n",
    "test_content, test_label=create_queue(test_contents,test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train,valid,test batch 객체 생성\n",
    "train_batch = tf.train.batch(\n",
    "                        [train_content, train_label],\n",
    "                        batch_size=TRAIN_BATCH_SIZE\n",
    "                    )\n",
    "\n",
    "valid_batch = tf.train.batch(\n",
    "                        [valid_content, valid_label],\n",
    "                        batch_size=VALID_BATCH_SIZE\n",
    "                    )\t\t\n",
    "\n",
    "test_batch = tf.train.batch(\n",
    "                        [test_content,test_label],\n",
    "                        batch_size=TEST_BATCH_SIZE\n",
    "                    )\t\t\t\n",
    "print (\"input pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch norm\n",
    "def Batch_Norm(x, training, scope=\"bn\"):\n",
    "    with arg_scope([batch_norm],\n",
    "                    scope=scope,\n",
    "                    updates_collections=None,\n",
    "                    decay=0.9,\n",
    "                    center=True,\n",
    "                    scale=True,\n",
    "                    zero_debias_moving_mean=True):\n",
    "        return tf.cond(training,\n",
    "                        lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                        lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "                       \n",
    "#BR layer\n",
    "def BR_Layer(x,name,output_num,training):\n",
    "    xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
    "    shape = x.get_shape().as_list()\n",
    "    input_num= shape[1]\n",
    "    W = tf.Variable(xavier_initializer([input_num, output_num]))\n",
    "    b = tf.Variable(xavier_initializer([output_num]))\n",
    "    _x = tf.matmul(x, W) + b\n",
    "    _x = Batch_Norm(_x,training, scope=\"bn_\"+name)\n",
    "    _x = tf.nn.relu(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "\n",
    "# placeholder is used for feeding data.\n",
    "x = tf.placeholder(tf.float32, shape=[None, NUM_DIM], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "y_target = tf.placeholder(tf.float32, shape=[None, NUM_LABELS], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
    "training=tf.placeholder(tf.bool)\n",
    "\n",
    "xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# reshape input data\n",
    "_x = tf.reshape(x,[-1,NUM_DIM],name=\"x_data\")\n",
    "_x=BR_Layer(_x,\"layer1\",32,training)\n",
    "_x=BR_Layer(_x,\"layer2\",16,training)\n",
    "_x=BR_Layer(_x,\"layer3\",32,training)\n",
    "\n",
    "shape = _x.get_shape().as_list()\n",
    "input_num= shape[1]                       \n",
    "W = tf.Variable(xavier_initializer([input_num, NUM_LABELS]))\n",
    "b = tf.Variable(xavier_initializer([NUM_LABELS]))\n",
    "_x = tf.matmul(_x, W) + b\n",
    "                       \n",
    "pred=_x\n",
    "prob_y=tf.nn.softmax(pred, name=\"prob_y\")\n",
    "\n",
    "# define the Loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target))\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define optimization algorithm\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y_target, 1))\n",
    "# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "\n",
    "#save weight\n",
    "t_vars = tf.trainable_variables()\n",
    "saver = tf.train.Saver(max_to_keep=None,var_list=t_vars)\n",
    "saver_def = saver.as_saver_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto(log_device_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM\n",
    "#config.operation_timeout_in_ms=60000   # terminate on long hangs\n",
    "configure=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True))\n",
    "\n",
    "sess = tf.Session(config=configure) \n",
    "\n",
    "training_loss=tf.summary.scalar('training_loss', cost)\n",
    "validation_loss=tf.summary.scalar('validation_loss', cost)\n",
    "\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "validation_accuracy = tf.summary.scalar(\"validation_accuracy\", accuracy)\n",
    "\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_op = tf.summary.merge_all()\n",
    "\n",
    "writer=tf.summary.FileWriter(LOGS_PATH, sess.graph)\n",
    "\n",
    "# initialization\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started!\n",
      "Epoch: 0000 cost= 4.639399743\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 5.254283 Validation-Accuracy: 0.33333334 Val-Loss: 5.215827 \n",
      "\n",
      "Epoch: 0001 cost= 0.969401014\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.80252856 Validation-Accuracy: 0.33333334 Val-Loss: 0.95295346 \n",
      "\n",
      "Epoch: 0002 cost= 0.974264097\n",
      "Train-Accuracy: 0.36956522 Train-Loss: 0.9641598 Validation-Accuracy: 0.26666668 Val-Loss: 1.192366 \n",
      "\n",
      "Epoch: 0003 cost= 0.980583316\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 1.0022396 Validation-Accuracy: 0.33333334 Val-Loss: 1.0413191 \n",
      "\n",
      "Epoch: 0004 cost= 0.974107856\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.9040375 Validation-Accuracy: 0.33333334 Val-Loss: 0.8269136 \n",
      "\n",
      "Epoch: 0005 cost= 0.953167403\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 1.0757074 Validation-Accuracy: 0.26666668 Val-Loss: 1.0450082 \n",
      "\n",
      "Epoch: 0006 cost= 0.955084419\n",
      "Train-Accuracy: 0.32608697 Train-Loss: 1.069664 Validation-Accuracy: 0.26666668 Val-Loss: 0.9918131 \n",
      "\n",
      "Epoch: 0007 cost= 0.942069858\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.90469974 Validation-Accuracy: 0.33333334 Val-Loss: 1.0356935 \n",
      "\n",
      "Epoch: 0008 cost= 0.930248672\n",
      "Train-Accuracy: 0.3478261 Train-Loss: 0.9929982 Validation-Accuracy: 0.2 Val-Loss: 1.3293816 \n",
      "\n",
      "Epoch: 0009 cost= 0.911948889\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.9237847 Validation-Accuracy: 0.33333334 Val-Loss: 0.9596662 \n",
      "\n",
      "Epoch: 0010 cost= 0.900383878\n",
      "Train-Accuracy: 0.3043478 Train-Loss: 0.9119284 Validation-Accuracy: 0.2 Val-Loss: 1.3098819 \n",
      "\n",
      "Epoch: 0011 cost= 0.883509994\n",
      "Train-Accuracy: 0.32608697 Train-Loss: 1.0621716 Validation-Accuracy: 0.4 Val-Loss: 0.8483999 \n",
      "\n",
      "Epoch: 0012 cost= 0.892946917\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.83724463 Validation-Accuracy: 0.4 Val-Loss: 0.97971743 \n",
      "\n",
      "Epoch: 0013 cost= 0.860021746\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.97097945 Validation-Accuracy: 0.46666667 Val-Loss: 0.8275948 \n",
      "\n",
      "Epoch: 0014 cost= 0.884869593\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.9069915 Validation-Accuracy: 0.26666668 Val-Loss: 1.1075112 \n",
      "\n",
      "Epoch: 0015 cost= 0.867490244\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 0.9914295 Validation-Accuracy: 0.46666667 Val-Loss: 0.81846964 \n",
      "\n",
      "Epoch: 0016 cost= 0.854392761\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.7388583 Validation-Accuracy: 0.4 Val-Loss: 1.0091611 \n",
      "\n",
      "Epoch: 0017 cost= 0.855080122\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.91740316 Validation-Accuracy: 0.2 Val-Loss: 0.9777518 \n",
      "\n",
      "Epoch: 0018 cost= 0.823822105\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 0.9576331 Validation-Accuracy: 0.4 Val-Loss: 0.9261192 \n",
      "\n",
      "Epoch: 0019 cost= 0.827365625\n",
      "Train-Accuracy: 0.45652175 Train-Loss: 0.8552227 Validation-Accuracy: 0.46666667 Val-Loss: 0.8246352 \n",
      "\n",
      "Epoch: 0020 cost= 0.818563139\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.7845938 Validation-Accuracy: 0.13333334 Val-Loss: 1.0561397 \n",
      "\n",
      "Epoch: 0021 cost= 0.833094543\n",
      "Train-Accuracy: 0.45652175 Train-Loss: 0.8367945 Validation-Accuracy: 0.2 Val-Loss: 0.90050566 \n",
      "\n",
      "Epoch: 0022 cost= 0.820338625\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.8609422 Validation-Accuracy: 0.6 Val-Loss: 0.85163325 \n",
      "\n",
      "Epoch: 0023 cost= 0.804780817\n",
      "Train-Accuracy: 0.45652175 Train-Loss: 0.7861 Validation-Accuracy: 0.46666667 Val-Loss: 0.88157374 \n",
      "\n",
      "Epoch: 0024 cost= 0.800606596\n",
      "Train-Accuracy: 0.36956522 Train-Loss: 0.9047207 Validation-Accuracy: 0.33333334 Val-Loss: 0.9560949 \n",
      "\n",
      "Epoch: 0025 cost= 0.792836660\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.7113933 Validation-Accuracy: 0.33333334 Val-Loss: 1.0552795 \n",
      "\n",
      "Epoch: 0026 cost= 0.793743461\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.79901123 Validation-Accuracy: 0.46666667 Val-Loss: 0.82936954 \n",
      "\n",
      "Epoch: 0027 cost= 0.778128111\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.8157342 Validation-Accuracy: 0.6 Val-Loss: 0.70808595 \n",
      "\n",
      "Epoch: 0028 cost= 0.764592391\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.80381835 Validation-Accuracy: 0.6666667 Val-Loss: 0.61743027 \n",
      "\n",
      "Epoch: 0029 cost= 0.778580534\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 0.91209155 Validation-Accuracy: 0.46666667 Val-Loss: 0.80374056 \n",
      "\n",
      "Epoch: 0030 cost= 0.763261533\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.77857655 Validation-Accuracy: 0.33333334 Val-Loss: 1.006561 \n",
      "\n",
      "Epoch: 0031 cost= 0.757986438\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.7649329 Validation-Accuracy: 0.46666667 Val-Loss: 0.80210096 \n",
      "\n",
      "Epoch: 0032 cost= 0.753820783\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.66102934 Validation-Accuracy: 0.4 Val-Loss: 0.74666905 \n",
      "\n",
      "Epoch: 0033 cost= 0.754173142\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.7773599 Validation-Accuracy: 0.33333334 Val-Loss: 0.88670355 \n",
      "\n",
      "Epoch: 0034 cost= 0.744193619\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.7703236 Validation-Accuracy: 0.6 Val-Loss: 0.7192658 \n",
      "\n",
      "Epoch: 0035 cost= 0.753453034\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.6002412 Validation-Accuracy: 0.46666667 Val-Loss: 0.7220275 \n",
      "\n",
      "Epoch: 0036 cost= 0.726460510\n",
      "Train-Accuracy: 0.45652175 Train-Loss: 0.8604198 Validation-Accuracy: 0.53333336 Val-Loss: 0.8293316 \n",
      "\n",
      "Epoch: 0037 cost= 0.739684480\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.8091846 Validation-Accuracy: 0.33333334 Val-Loss: 0.87685984 \n",
      "\n",
      "Epoch: 0038 cost= 0.730056089\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.68599826 Validation-Accuracy: 0.73333335 Val-Loss: 0.6668715 \n",
      "\n",
      "Epoch: 0039 cost= 0.719112951\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.7767344 Validation-Accuracy: 0.6 Val-Loss: 0.8061235 \n",
      "\n",
      "Epoch: 0040 cost= 0.714423472\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.645676 Validation-Accuracy: 0.6 Val-Loss: 0.72438455 \n",
      "\n",
      "Epoch: 0041 cost= 0.722798663\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 0.75684106 Validation-Accuracy: 0.6 Val-Loss: 0.73430735 \n",
      "\n",
      "Epoch: 0042 cost= 0.697839528\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.6653033 Validation-Accuracy: 0.6666667 Val-Loss: 0.65932745 \n",
      "\n",
      "Epoch: 0043 cost= 0.700980067\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.6396617 Validation-Accuracy: 0.4 Val-Loss: 0.97926074 \n",
      "\n",
      "Epoch: 0044 cost= 0.688807958\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.6215719 Validation-Accuracy: 0.4 Val-Loss: 0.8419034 \n",
      "\n",
      "Epoch: 0045 cost= 0.702810556\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.6499654 Validation-Accuracy: 0.6 Val-Loss: 0.66618514 \n",
      "\n",
      "Epoch: 0046 cost= 0.685226083\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.7724614 Validation-Accuracy: 0.53333336 Val-Loss: 0.6786447 \n",
      "\n",
      "Epoch: 0047 cost= 0.688586533\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.70468885 Validation-Accuracy: 0.46666667 Val-Loss: 0.7961403 \n",
      "\n",
      "Epoch: 0048 cost= 0.679077387\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.8287455 Validation-Accuracy: 0.73333335 Val-Loss: 0.6519372 \n",
      "\n",
      "Epoch: 0049 cost= 0.680062801\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.7806073 Validation-Accuracy: 0.6 Val-Loss: 0.7041665 \n",
      "\n",
      "Epoch: 0050 cost= 0.671925855\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.67300117 Validation-Accuracy: 0.6 Val-Loss: 0.6676952 \n",
      "\n",
      "Epoch: 0051 cost= 0.678398174\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.6233054 Validation-Accuracy: 0.53333336 Val-Loss: 0.7037107 \n",
      "\n",
      "Epoch: 0052 cost= 0.664702487\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.648525 Validation-Accuracy: 0.53333336 Val-Loss: 0.701953 \n",
      "\n",
      "Epoch: 0053 cost= 0.663942027\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.74365187 Validation-Accuracy: 0.6666667 Val-Loss: 0.55049396 \n",
      "\n",
      "Epoch: 0054 cost= 0.659272885\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.5730246 Validation-Accuracy: 0.6666667 Val-Loss: 0.6534493 \n",
      "\n",
      "Epoch: 0055 cost= 0.656801397\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.7638836 Validation-Accuracy: 0.46666667 Val-Loss: 0.95354533 \n",
      "\n",
      "Epoch: 0056 cost= 0.652263981\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.7173174 Validation-Accuracy: 0.6 Val-Loss: 0.59721404 \n",
      "\n",
      "Epoch: 0057 cost= 0.657435066\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.7192683 Validation-Accuracy: 0.53333336 Val-Loss: 0.8054291 \n",
      "\n",
      "Epoch: 0058 cost= 0.659497744\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.5889757 Validation-Accuracy: 0.6666667 Val-Loss: 0.6201118 \n",
      "\n",
      "Epoch: 0059 cost= 0.653285319\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.7220041 Validation-Accuracy: 0.6666667 Val-Loss: 0.59800786 \n",
      "\n",
      "Epoch: 0060 cost= 0.638460314\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.73203325 Validation-Accuracy: 0.6666667 Val-Loss: 0.5979015 \n",
      "\n",
      "Epoch: 0061 cost= 0.648662043\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.5969783 Validation-Accuracy: 0.4 Val-Loss: 0.82264584 \n",
      "\n",
      "Epoch: 0062 cost= 0.642902756\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.63168633 Validation-Accuracy: 0.6666667 Val-Loss: 0.62572044 \n",
      "\n",
      "Epoch: 0063 cost= 0.648175573\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.568412 Validation-Accuracy: 0.6 Val-Loss: 0.6636387 \n",
      "\n",
      "Epoch: 0064 cost= 0.617367339\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.59466004 Validation-Accuracy: 0.73333335 Val-Loss: 0.525191 \n",
      "\n",
      "Epoch: 0065 cost= 0.634485877\n",
      "Train-Accuracy: 0.7173913 Train-Loss: 0.6573152 Validation-Accuracy: 0.8 Val-Loss: 0.5144925 \n",
      "\n",
      "Epoch: 0066 cost= 0.625034600\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.6740128 Validation-Accuracy: 0.53333336 Val-Loss: 0.86650467 \n",
      "\n",
      "Epoch: 0067 cost= 0.627520287\n",
      "Train-Accuracy: 0.7173913 Train-Loss: 0.5728973 Validation-Accuracy: 0.46666667 Val-Loss: 0.77985007 \n",
      "\n",
      "Epoch: 0068 cost= 0.619618326\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.6957346 Validation-Accuracy: 0.8 Val-Loss: 0.49368486 \n",
      "\n",
      "Epoch: 0069 cost= 0.620985037\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.62479734 Validation-Accuracy: 0.73333335 Val-Loss: 0.58397335 \n",
      "\n",
      "Epoch: 0070 cost= 0.613936979\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.65106064 Validation-Accuracy: 0.6 Val-Loss: 0.73075956 \n",
      "\n",
      "(result)test accuracy: 0.466667 / weight-65\n",
      "close\n"
     ]
    }
   ],
   "source": [
    "print(\"Session started!\")\n",
    "start_session_time = time.time()\n",
    "sess.run(init_op)\n",
    "\n",
    "# initialize the queue threads to start to shovel data\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "max_epoch=MAX_EPOCH\n",
    "display_step=1\n",
    "max_accuracy=0\n",
    "max_index=0\n",
    "for epoch in range(0,max_epoch+1):\n",
    "    avg_cost = 0\n",
    "    total_batch=(int)(total_train_size/TRAIN_BATCH_SIZE)\n",
    "    for step in range(total_batch):\n",
    "        _train_batch=sess.run([tf.cast(train_batch[0], tf.float32),tf.cast(train_batch[1], tf.float32)])\n",
    "        if epoch!=0:\n",
    "            _,c=sess.run([train_step,cost] , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: True})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        else:\n",
    "            c=sess.run(cost , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: False})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "\n",
    "        print (\"Epoch:\", '%04d' % (epoch), \"cost=\",\"{:.9f}\".format(avg_cost))\n",
    "\n",
    "        _valid_batch=sess.run([tf.cast(valid_batch[0], tf.float32),tf.cast(valid_batch[1], tf.float32)])\n",
    "\n",
    "        # traininig accuracy.\n",
    "        train_cos,train_acc, train_summ_acc,train_summ_loss = sess.run(\n",
    "        [cost, accuracy, training_accuracy,training_loss], \n",
    "        feed_dict={x : _train_batch[0],  y_target : _train_batch[1],training: False})\n",
    "        writer.add_summary(train_summ_acc, epoch) \n",
    "        writer.add_summary(train_summ_loss, epoch) \n",
    "\n",
    "\n",
    "        # validation accuracy.\n",
    "        valid_cos, valid_acc, valid_summ_acc,valid_summ_loss  = sess.run(\n",
    "        [cost,accuracy, validation_accuracy,validation_loss],\n",
    "        feed_dict={x: _valid_batch[0], y_target: _valid_batch[1],training: False})\n",
    "        writer.add_summary(valid_summ_acc, epoch)\n",
    "        writer.add_summary(valid_summ_loss, epoch)\n",
    "\n",
    "        saver.save(sess,\"./weight/w\",epoch)\n",
    "        print(\"Train-Accuracy:\", train_acc,\"Train-Loss:\", train_cos, \"Validation-Accuracy:\", valid_acc,\"Val-Loss:\", valid_cos,\"\\n\")\n",
    "        if valid_acc > max_accuracy:\n",
    "            max_accuracy=valid_acc\n",
    "            max_index=epoch\n",
    "\n",
    "\n",
    "\n",
    "saver.restore(sess,\"./weight/w-%d\"%(max_index))\n",
    "_test_batch=sess.run([tf.cast(test_batch[0],tf.float32),tf.cast(test_batch[1], tf.float32)])\n",
    "_accuracy=sess.run(accuracy, feed_dict={x: _test_batch[0], y_target:  _test_batch[1],training: False})\n",
    "\n",
    "print(\"(result)test accuracy: %g / weight-%d\"%(_accuracy,max_index))    \n",
    "\n",
    "# stop our queue threads and properly close the session\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "writer.close()\n",
    "sess.close()\n",
    "\n",
    "print(\"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
