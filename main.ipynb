{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import msvcrt\n",
    "import sys\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#데이터셋 경로 위치\n",
    "DATASET_PATH      = \"./diabetes.csv\"\n",
    "#텐서보드 저장경로\n",
    "LOGS_PATH\t= \"logs/\"\n",
    "#모델의 입력 차원=> 현재 input pipeline에서 자동 조정 \n",
    "NUM_DIM=8\n",
    "NUM_LABELS\t\t= 2\n",
    "#batch size 모두 0 => input pipeline에서 자동 조정 \n",
    "TRAIN_BATCH_SIZE\t= 0\n",
    "VALID_BATCH_SIZE\t= 0\n",
    "TEST_BATCH_SIZE\t\t= 0\n",
    "#epoch \n",
    "MAX_EPOCH=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(라벨,콘텐츠(etc 음성,텍스트)) 데이터 읽기\t=> trainset,validset, test으로 나뉘어짐 / ratio는 다 합쳐서 1이 되어야 됌 \n",
    "def read_label_file(file='./diabetes.csv',train_split_ratio=0.6,valid_split_ratio=0.2,test_split_ratio=0.2):\n",
    "\tdf = pd.read_csv(file, encoding='latin-1')\n",
    "\tlabel=df.Outcome.tolist()\n",
    "\tcontents=[]\n",
    "\tfor row in df.iterrows():\n",
    "\t\tindex, data = row\n",
    "\t\tcontents.append(data[0:8].tolist())\n",
    "\n",
    "\tsize=len(label)\n",
    "\n",
    "\tsum_ratio=train_split_ratio+valid_split_ratio+test_split_ratio\n",
    "\t\n",
    "\tassert  abs(1.0000 - sum_ratio) <= 0.0001, 'ratio의 합(%f)이 1이 아닙니다.'%(sum_ratio) \n",
    "    \n",
    "\ttrain_size=int(size*train_split_ratio)\n",
    "\ttrain_split_range=train_size\n",
    "\ttrain_contents=contents[0:train_split_range]\n",
    "\ttrain_label=label[0:train_split_range]\n",
    "\t\n",
    "\tvalid_size=int(size*valid_split_ratio)\n",
    "\tvalid_split_range=valid_size+train_split_range\n",
    "\tvalid_contents=contents[train_split_range:valid_split_range]\n",
    "\tvalid_label=label[train_split_range:valid_split_range]\n",
    "\t\n",
    "\ttest_size=int(size*test_split_ratio)\n",
    "\ttest_split_range=test_size+valid_split_range\n",
    "\ttest_contents=contents[valid_split_range:test_split_range]\n",
    "\ttest_label=label[valid_split_range:test_split_range]\n",
    "\t\n",
    "\t\n",
    "\treturn train_size,valid_size,test_size,train_contents,train_label,valid_contents,valid_label,test_contents,test_label\n",
    "\n",
    "def preprocess(data,label,num_class):\n",
    "\n",
    "\t#data를 sequences로 변환\n",
    "\tcontents=data\n",
    "\t\n",
    "\t# string을 tensor로 변환, type=float32\n",
    "\tlabel = tf.one_hot(label,depth=num_class,on_value=1,off_value=0,axis=-1)\n",
    "\treturn contents,label\n",
    "\t\t\n",
    "def create_queue(contents,labels):\n",
    "\n",
    "\t#입력 큐 생성, 데이터 섞기(shuffle)=true \n",
    "\tinput_queue = tf.train.slice_input_producer(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t[contents, labels],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tshuffle=True)\n",
    "\t#입력 큐 대입(데이터,라벨)\n",
    "\tcontent = input_queue[0]\n",
    "\tlabel = input_queue[1]\n",
    "\t\n",
    "\treturn content,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input pipeline ready\n"
     ]
    }
   ],
   "source": [
    "#read_label_file_split(dataset_path)\n",
    "total_train_size,total_valid_size,total_test_size,train_contents,train_labels,valid_contents,valid_labels,test_contents,test_labels = read_label_file(DATASET_PATH,0.6,0.2,0.2)\n",
    "#batch size  = 전체 데이터셋 수의 10%으로 설정 \n",
    "if(TRAIN_BATCH_SIZE==0):\n",
    "    TRAIN_BATCH_SIZE=(int)(total_train_size*0.1)\n",
    "if(VALID_BATCH_SIZE==0):\n",
    "    VALID_BATCH_SIZE=(int)(total_valid_size*0.1) \n",
    "if(TEST_BATCH_SIZE==0):\n",
    "    TEST_BATCH_SIZE=(int)(total_test_size*0.1) \n",
    "\n",
    "#train,valid,test set 각각 전처리\n",
    "train_contents,train_labels=preprocess(train_contents,train_labels,NUM_LABELS)\n",
    "valid_contents,valid_labels=preprocess(valid_contents,valid_labels,NUM_LABELS)\n",
    "test_contents,test_labels=preprocess(test_contents,test_labels,NUM_LABELS)\n",
    "\n",
    "\n",
    "\n",
    "#큐 생성  => train,valid,test  => train,valid,test set 각각 큐 생성 \n",
    "train_content, train_label=create_queue(train_contents,train_labels)\n",
    "valid_content, valid_label=create_queue(valid_contents,valid_labels)\n",
    "test_content, test_label=create_queue(test_contents,test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train,valid,test batch 객체 생성\n",
    "train_batch = tf.train.batch(\n",
    "                        [train_content, train_label],\n",
    "                        batch_size=TRAIN_BATCH_SIZE\n",
    "                    )\n",
    "\n",
    "valid_batch = tf.train.batch(\n",
    "                        [valid_content, valid_label],\n",
    "                        batch_size=VALID_BATCH_SIZE\n",
    "                    )\t\t\n",
    "\n",
    "test_batch = tf.train.batch(\n",
    "                        [test_content,test_label],\n",
    "                        batch_size=TEST_BATCH_SIZE\n",
    "                    )\t\t\t\n",
    "print (\"input pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch norm\n",
    "def Batch_Norm(x, training, scope=\"bn\"):\n",
    "    with arg_scope([batch_norm],\n",
    "                    scope=scope,\n",
    "                    updates_collections=None,\n",
    "                    decay=0.9,\n",
    "                    center=True,\n",
    "                    scale=True,\n",
    "                    zero_debias_moving_mean=True):\n",
    "        return tf.cond(training,\n",
    "                        lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                        lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "                       \n",
    "#BR layer\n",
    "def BR_Layer(x,name,output_num,training):\n",
    "    xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
    "    shape = x.get_shape().as_list()\n",
    "    input_num= shape[1]\n",
    "    W = tf.Variable(xavier_initializer([input_num, output_num]))\n",
    "    b = tf.Variable(xavier_initializer([output_num]))\n",
    "    _x = tf.matmul(x, W) + b\n",
    "    _x = Batch_Norm(_x,training, scope=\"bn_\"+name)\n",
    "    _x = tf.nn.relu(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "\n",
    "# placeholder is used for feeding data.\n",
    "x = tf.placeholder(tf.float32, shape=[None, NUM_DIM], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "y_target = tf.placeholder(tf.float32, shape=[None, NUM_LABELS], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
    "training=tf.placeholder(tf.bool)\n",
    "\n",
    "xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# reshape input data\n",
    "_x = tf.reshape(x,[-1,NUM_DIM],name=\"x_data\")\n",
    "_x=BR_Layer(_x,\"layer1\",32,training)\n",
    "_x=BR_Layer(_x,\"layer2\",16,training)\n",
    "_x=BR_Layer(_x,\"layer3\",32,training)\n",
    "\n",
    "shape = _x.get_shape().as_list()\n",
    "input_num= shape[1]                       \n",
    "W = tf.Variable(xavier_initializer([input_num, NUM_LABELS]))\n",
    "b = tf.Variable(xavier_initializer([NUM_LABELS]))\n",
    "_x = tf.matmul(_x, W) + b\n",
    "                       \n",
    "pred=_x\n",
    "prob_y=tf.nn.softmax(pred, name=\"prob_y\")\n",
    "\n",
    "# define the Loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target))\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define optimization algorithm\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y_target, 1))\n",
    "# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "\n",
    "#save weight\n",
    "t_vars = tf.trainable_variables()\n",
    "saver = tf.train.Saver(max_to_keep=None,var_list=t_vars)\n",
    "saver_def = saver.as_saver_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto(log_device_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM\n",
    "#config.operation_timeout_in_ms=60000   # terminate on long hangs\n",
    "configure=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True))\n",
    "\n",
    "sess = tf.Session(config=configure) \n",
    "\n",
    "training_loss=tf.summary.scalar('training_loss', cost)\n",
    "validation_loss=tf.summary.scalar('validation_loss', cost)\n",
    "\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "validation_accuracy = tf.summary.scalar(\"validation_accuracy\", accuracy)\n",
    "\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_op = tf.summary.merge_all()\n",
    "\n",
    "writer=tf.summary.FileWriter(LOGS_PATH, sess.graph)\n",
    "\n",
    "# initialization\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started!\n",
      "Epoch: 0000 cost= 6.664971304\n",
      "Train-Accuracy: 0.3478261 Train-Loss: 5.732937 Validation-Accuracy: 0.2 Val-Loss: 6.8177023 \n",
      "\n",
      "Epoch: 0001 cost= 1.251628613\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 1.1713648 Validation-Accuracy: 0.06666667 Val-Loss: 1.6475751 \n",
      "\n",
      "Epoch: 0002 cost= 1.216408736\n",
      "Train-Accuracy: 0.36956522 Train-Loss: 1.132679 Validation-Accuracy: 0.06666667 Val-Loss: 1.6412461 \n",
      "\n",
      "Epoch: 0003 cost= 1.194443023\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 1.1605194 Validation-Accuracy: 0.26666668 Val-Loss: 1.5236717 \n",
      "\n",
      "Epoch: 0004 cost= 1.163214523\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 1.0068983 Validation-Accuracy: 0.26666668 Val-Loss: 1.3762877 \n",
      "\n",
      "Epoch: 0005 cost= 1.148705971\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 1.1113482 Validation-Accuracy: 0.6 Val-Loss: 0.75704056 \n",
      "\n",
      "Epoch: 0006 cost= 1.107149655\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 1.134908 Validation-Accuracy: 0.46666667 Val-Loss: 1.0282822 \n",
      "\n",
      "Epoch: 0007 cost= 1.078127372\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 0.9020238 Validation-Accuracy: 0.4 Val-Loss: 1.0967695 \n",
      "\n",
      "Epoch: 0008 cost= 1.065842122\n",
      "Train-Accuracy: 0.3478261 Train-Loss: 1.1683034 Validation-Accuracy: 0.26666668 Val-Loss: 1.379435 \n",
      "\n",
      "Epoch: 0009 cost= 1.035954535\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.7412567 Validation-Accuracy: 0.4 Val-Loss: 1.1329128 \n",
      "\n",
      "Epoch: 0010 cost= 1.023007601\n",
      "Train-Accuracy: 0.3478261 Train-Loss: 1.1420212 Validation-Accuracy: 0.4 Val-Loss: 0.9951227 \n",
      "\n",
      "Epoch: 0011 cost= 1.010499424\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.87239224 Validation-Accuracy: 0.46666667 Val-Loss: 1.0940334 \n",
      "\n",
      "Epoch: 0012 cost= 0.983548152\n",
      "Train-Accuracy: 0.39130434 Train-Loss: 0.91856635 Validation-Accuracy: 0.13333334 Val-Loss: 1.2667196 \n",
      "\n",
      "Epoch: 0013 cost= 0.982711309\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.7240171 Validation-Accuracy: 0.33333334 Val-Loss: 0.90785474 \n",
      "\n",
      "Epoch: 0014 cost= 0.952713174\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.9105522 Validation-Accuracy: 0.53333336 Val-Loss: 0.80878854 \n",
      "\n",
      "Epoch: 0015 cost= 0.937578940\n",
      "Train-Accuracy: 0.45652175 Train-Loss: 0.8830247 Validation-Accuracy: 0.33333334 Val-Loss: 1.0864408 \n",
      "\n",
      "Epoch: 0016 cost= 0.925524801\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.8365547 Validation-Accuracy: 0.33333334 Val-Loss: 1.1835684 \n",
      "\n",
      "Epoch: 0017 cost= 0.906469131\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 0.9125371 Validation-Accuracy: 0.13333334 Val-Loss: 1.2372602 \n",
      "\n",
      "Epoch: 0018 cost= 0.891073090\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.80024993 Validation-Accuracy: 0.6 Val-Loss: 0.88027024 \n",
      "\n",
      "Epoch: 0019 cost= 0.885514098\n",
      "Train-Accuracy: 0.36956522 Train-Loss: 1.0051425 Validation-Accuracy: 0.33333334 Val-Loss: 0.8609773 \n",
      "\n",
      "Epoch: 0020 cost= 0.858556634\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.8902049 Validation-Accuracy: 0.2 Val-Loss: 1.0242391 \n",
      "\n",
      "Epoch: 0021 cost= 0.859727329\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.82721305 Validation-Accuracy: 0.26666668 Val-Loss: 1.3421962 \n",
      "\n",
      "Epoch: 0022 cost= 0.847446138\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.77289903 Validation-Accuracy: 0.6 Val-Loss: 0.7036235 \n",
      "\n",
      "Epoch: 0023 cost= 0.832872838\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.7862043 Validation-Accuracy: 0.4 Val-Loss: 0.84973943 \n",
      "\n",
      "Epoch: 0024 cost= 0.819467425\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.87162507 Validation-Accuracy: 0.4 Val-Loss: 0.88296205 \n",
      "\n",
      "Epoch: 0025 cost= 0.818875527\n",
      "Train-Accuracy: 0.41304347 Train-Loss: 1.0820405 Validation-Accuracy: 0.4 Val-Loss: 0.92350984 \n",
      "\n",
      "Epoch: 0026 cost= 0.792884398\n",
      "Train-Accuracy: 0.47826087 Train-Loss: 1.0009756 Validation-Accuracy: 0.4 Val-Loss: 0.88689315 \n",
      "\n",
      "Epoch: 0027 cost= 0.789038616\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.8530758 Validation-Accuracy: 0.53333336 Val-Loss: 0.6082792 \n",
      "\n",
      "Epoch: 0028 cost= 0.780870682\n",
      "Train-Accuracy: 0.5217391 Train-Loss: 0.71954805 Validation-Accuracy: 0.4 Val-Loss: 0.79351526 \n",
      "\n",
      "Epoch: 0029 cost= 0.765885127\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.79983187 Validation-Accuracy: 0.46666667 Val-Loss: 0.7381441 \n",
      "\n",
      "Epoch: 0030 cost= 0.764934325\n",
      "Train-Accuracy: 0.4347826 Train-Loss: 0.941234 Validation-Accuracy: 0.46666667 Val-Loss: 0.7774242 \n",
      "\n",
      "Epoch: 0031 cost= 0.753705627\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.73347616 Validation-Accuracy: 0.53333336 Val-Loss: 0.6504051 \n",
      "\n",
      "Epoch: 0032 cost= 0.725775301\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.64547634 Validation-Accuracy: 0.53333336 Val-Loss: 0.62812006 \n",
      "\n",
      "Epoch: 0033 cost= 0.718697965\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.6347104 Validation-Accuracy: 0.4 Val-Loss: 0.8292231 \n",
      "\n",
      "Epoch: 0034 cost= 0.716932291\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.6198842 Validation-Accuracy: 0.46666667 Val-Loss: 0.8065078 \n",
      "\n",
      "Epoch: 0035 cost= 0.713883448\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.7445291 Validation-Accuracy: 0.53333336 Val-Loss: 0.81308156 \n",
      "\n",
      "Epoch: 0036 cost= 0.715515721\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.6615409 Validation-Accuracy: 0.6 Val-Loss: 0.71286076 \n",
      "\n",
      "Epoch: 0037 cost= 0.700933850\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.6363939 Validation-Accuracy: 0.53333336 Val-Loss: 0.6252126 \n",
      "\n",
      "Epoch: 0038 cost= 0.708589000\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.62022454 Validation-Accuracy: 0.53333336 Val-Loss: 0.8607809 \n",
      "\n",
      "Epoch: 0039 cost= 0.690817237\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.67436033 Validation-Accuracy: 0.6 Val-Loss: 0.71886295 \n",
      "\n",
      "Epoch: 0040 cost= 0.680444896\n",
      "Train-Accuracy: 0.54347825 Train-Loss: 0.71230584 Validation-Accuracy: 0.53333336 Val-Loss: 0.8171548 \n",
      "\n",
      "Epoch: 0041 cost= 0.692567003\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.5595429 Validation-Accuracy: 0.6 Val-Loss: 0.7013119 \n",
      "\n",
      "Epoch: 0042 cost= 0.661035621\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.6804959 Validation-Accuracy: 0.6 Val-Loss: 0.74208796 \n",
      "\n",
      "Epoch: 0043 cost= 0.664004922\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.6460796 Validation-Accuracy: 0.46666667 Val-Loss: 0.7054623 \n",
      "\n",
      "Epoch: 0044 cost= 0.675351292\n",
      "Train-Accuracy: 0.73913044 Train-Loss: 0.57989603 Validation-Accuracy: 0.46666667 Val-Loss: 0.7409432 \n",
      "\n",
      "Epoch: 0045 cost= 0.660624391\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.60981804 Validation-Accuracy: 0.46666667 Val-Loss: 0.7231645 \n",
      "\n",
      "Epoch: 0046 cost= 0.622889674\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.6218789 Validation-Accuracy: 0.6666667 Val-Loss: 0.49240854 \n",
      "\n",
      "Epoch: 0047 cost= 0.639487392\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.64195055 Validation-Accuracy: 0.8 Val-Loss: 0.5748301 \n",
      "\n",
      "Epoch: 0048 cost= 0.625039661\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.5962468 Validation-Accuracy: 0.46666667 Val-Loss: 0.69679207 \n",
      "\n",
      "Epoch: 0049 cost= 0.619458467\n",
      "Train-Accuracy: 0.6304348 Train-Loss: 0.69535154 Validation-Accuracy: 0.6666667 Val-Loss: 0.68509424 \n",
      "\n",
      "Epoch: 0050 cost= 0.623703033\n",
      "Train-Accuracy: 0.7173913 Train-Loss: 0.5270909 Validation-Accuracy: 0.73333335 Val-Loss: 0.54678357 \n",
      "\n",
      "Epoch: 0051 cost= 0.612615675\n",
      "Train-Accuracy: 0.5 Train-Loss: 0.7864905 Validation-Accuracy: 0.4 Val-Loss: 0.8857651 \n",
      "\n",
      "Epoch: 0052 cost= 0.627369034\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.65298593 Validation-Accuracy: 0.6 Val-Loss: 0.614875 \n",
      "\n",
      "Epoch: 0053 cost= 0.636325288\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.592379 Validation-Accuracy: 0.6666667 Val-Loss: 0.61122036 \n",
      "\n",
      "Epoch: 0054 cost= 0.613770553\n",
      "Train-Accuracy: 0.76086956 Train-Loss: 0.52515835 Validation-Accuracy: 0.53333336 Val-Loss: 0.7529603 \n",
      "\n",
      "Epoch: 0055 cost= 0.601815897\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.59615564 Validation-Accuracy: 0.8 Val-Loss: 0.49479306 \n",
      "\n",
      "Epoch: 0056 cost= 0.603144431\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.59807974 Validation-Accuracy: 0.73333335 Val-Loss: 0.5312264 \n",
      "\n",
      "Epoch: 0057 cost= 0.595648932\n",
      "Train-Accuracy: 0.67391306 Train-Loss: 0.59972584 Validation-Accuracy: 0.8 Val-Loss: 0.4380165 \n",
      "\n",
      "Epoch: 0058 cost= 0.594239137\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.67575914 Validation-Accuracy: 0.6666667 Val-Loss: 0.5678034 \n",
      "\n",
      "Epoch: 0059 cost= 0.608263808\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.55970514 Validation-Accuracy: 0.73333335 Val-Loss: 0.5538285 \n",
      "\n",
      "Epoch: 0060 cost= 0.587427640\n",
      "Train-Accuracy: 0.5652174 Train-Loss: 0.6406824 Validation-Accuracy: 0.73333335 Val-Loss: 0.56687546 \n",
      "\n",
      "Epoch: 0061 cost= 0.586767355\n",
      "Train-Accuracy: 0.7173913 Train-Loss: 0.50334054 Validation-Accuracy: 0.8 Val-Loss: 0.47970143 \n",
      "\n",
      "Epoch: 0062 cost= 0.579226077\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.5616461 Validation-Accuracy: 0.6666667 Val-Loss: 0.64650816 \n",
      "\n",
      "Epoch: 0063 cost= 0.583347487\n",
      "Train-Accuracy: 0.65217394 Train-Loss: 0.63336027 Validation-Accuracy: 0.6666667 Val-Loss: 0.51467204 \n",
      "\n",
      "Epoch: 0064 cost= 0.579763782\n",
      "Train-Accuracy: 0.6086956 Train-Loss: 0.6897408 Validation-Accuracy: 0.8 Val-Loss: 0.5169227 \n",
      "\n",
      "Epoch: 0065 cost= 0.572476950\n",
      "Train-Accuracy: 0.73913044 Train-Loss: 0.469861 Validation-Accuracy: 0.8 Val-Loss: 0.58094543 \n",
      "\n",
      "Epoch: 0066 cost= 0.567179614\n",
      "Train-Accuracy: 0.5869565 Train-Loss: 0.6810425 Validation-Accuracy: 0.8 Val-Loss: 0.47521117 \n",
      "\n",
      "Epoch: 0067 cost= 0.578706452\n",
      "Train-Accuracy: 0.6956522 Train-Loss: 0.6035879 Validation-Accuracy: 0.46666667 Val-Loss: 0.5894306 \n",
      "\n",
      "Epoch: 0068 cost= 0.574427181\n",
      "Train-Accuracy: 0.82608694 Train-Loss: 0.49264506 Validation-Accuracy: 0.53333336 Val-Loss: 0.6891987 \n",
      "\n",
      "Epoch: 0069 cost= 0.557742459\n",
      "Train-Accuracy: 0.82608694 Train-Loss: 0.4863845 Validation-Accuracy: 0.53333336 Val-Loss: 0.69938314 \n",
      "\n",
      "Epoch: 0070 cost= 0.549511155\n",
      "Train-Accuracy: 0.82608694 Train-Loss: 0.5122941 Validation-Accuracy: 0.8 Val-Loss: 0.5000259 \n",
      "\n",
      "(result)test accuracy: 0.933333 / weight-47\n",
      "close\n"
     ]
    }
   ],
   "source": [
    "print(\"Session started!\")\n",
    "start_session_time = time.time()\n",
    "sess.run(init_op)\n",
    "\n",
    "# initialize the queue threads to start to shovel data\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "max_epoch=MAX_EPOCH\n",
    "display_step=1\n",
    "max_accuracy=0\n",
    "max_index=0\n",
    "for epoch in range(0,max_epoch+1):\n",
    "    avg_cost = 0\n",
    "    total_batch=(int)(total_train_size/TRAIN_BATCH_SIZE)\n",
    "    for step in range(total_batch):\n",
    "        _train_batch=sess.run([tf.cast(train_batch[0], tf.float32),tf.cast(train_batch[1], tf.float32)])\n",
    "        if epoch!=0:\n",
    "            _,c=sess.run([train_step,cost] , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: True})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        else:\n",
    "            c=sess.run(cost , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: False})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "\n",
    "        print (\"Epoch:\", '%04d' % (epoch), \"cost=\",\"{:.9f}\".format(avg_cost))\n",
    "\n",
    "        _valid_batch=sess.run([tf.cast(valid_batch[0], tf.float32),tf.cast(valid_batch[1], tf.float32)])\n",
    "\n",
    "        # traininig accuracy.\n",
    "        train_cos,train_acc, train_summ_acc,train_summ_loss = sess.run(\n",
    "        [cost, accuracy, training_accuracy,training_loss], \n",
    "        feed_dict={x : _train_batch[0],  y_target : _train_batch[1],training: False})\n",
    "        writer.add_summary(train_summ_acc, epoch) \n",
    "        writer.add_summary(train_summ_loss, epoch) \n",
    "\n",
    "\n",
    "        # validation accuracy.\n",
    "        valid_cos, valid_acc, valid_summ_acc,valid_summ_loss  = sess.run(\n",
    "        [cost,accuracy, validation_accuracy,validation_loss],\n",
    "        feed_dict={x: _valid_batch[0], y_target: _valid_batch[1],training: False})\n",
    "        writer.add_summary(valid_summ_acc, epoch)\n",
    "        writer.add_summary(valid_summ_loss, epoch)\n",
    "\n",
    "        saver.save(sess,\"./weight/w\",epoch)\n",
    "        print(\"Train-Accuracy:\", train_acc,\"Train-Loss:\", train_cos, \"Validation-Accuracy:\", valid_acc,\"Val-Loss:\", valid_cos,\"\\n\")\n",
    "        if valid_acc > max_accuracy:\n",
    "            max_accuracy=valid_acc\n",
    "            max_index=epoch\n",
    "\n",
    "\n",
    "\n",
    "saver.restore(sess,\"./weight/w-%d\"%(max_index))\n",
    "_test_batch=sess.run([tf.cast(test_batch[0],tf.float32),tf.cast(test_batch[1], tf.float32)])\n",
    "_accuracy=sess.run(accuracy, feed_dict={x: _test_batch[0], y_target:  _test_batch[1],training: False})\n",
    "\n",
    "print(\"(result)test accuracy: %g / weight-%d\"%(_accuracy,max_index))    \n",
    "\n",
    "# stop our queue threads and properly close the session\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "writer.close()\n",
    "sess.close()\n",
    "\n",
    "print(\"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
